---
title: "P8105 Data Science I Homework 4"
author: Olivia Wang (hw2852)
output: github_document
date: "2022-11-16"
---

In preparation for the problems below, we will load the following libraries: 

```{r load_libraries}
library(tidyverse)
library(readxl)
library(dplyr)
library(p8105.datasets)
```

# Problem 1

## 1.1 Data Import

Below we import the data in individual spreadsheets contained in `./data/zip_data/`. To do this, we created a data frame including the list of all files in that directory and the complete path to each file. Next, we apply the `map` function over the paths and import data using the `read_csv` function. Finally, we apply the `unnest` function to the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/"),
    path = str_c("data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = data)
```

## 1.2 Tidying Data

The result of the previous code chunk is not tidy -- data are wide rather than long, and some important variables are included as parts of others. Below we tidy the data by applying string manipulations, converting from wide to long using `pivot_longer`, and selecting relevant variables for further analysis. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

## 1.3 Generating Spaghetti Plot

Using the tidied data generated above, we can create a spaghetti plot using `ggplot`, showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

The plot generated above suggests high within-subject correlation. More specifically, subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally do not change over time, but those in the experiment group increase their outcome in a roughly linear way. 

# Problem 2

Let us begin by importing the CSV file containing _Washington Post's_ homicide data downloaded from GitHub, and applying the `clean_names` function. We can then apply the `skim` function to generate a brief data summary. 

```{r}
homicide_data = 
  read_csv("./homicide-data.csv") %>% 
  janitor::clean_names()

skimr::skim(homicide_data)
```

The _Washington Post's_ raw homicide data contains __`r nrow(homicide_data)` rows__ and __`r ncol(homicide_data)` columns__. Each row corresponds to one homicide case, and the following information regarding the homicide are found in each column:

* Case ID code
* Reported date
* Victim identifiers (name, age, race and sex)
* Location details (state, city, latitude and longitude)
* Disposition

Based on the output generated above, we can see that there are no missing values for all variables except for `latitude` and `longitude`. The `latitude` variable has __`r sum(is.na(homicide_data$lat))` missing values__ and the `longitude` variable has __`r sum(is.na(homicide_data$lon))` missing values__. 

## 2.1 City-Level Summaries of Homicide Data

#### _Total Homicides per City_

In the following code chunk, we create a new `city_state` variable (e.g. "Baltimore, MD") by joining the existing city and state variables using the `paste` command. Since each row represents a unique homicide case, we can determine the total number of homicides by counting the number of observations per city-state pair. We will save the output as a new data frame, to be used in further analysis. 

```{r}
homicide_data = homicide_data %>% 
  mutate(city_state = as.character(paste(city, state, sep = ", ")))

total_homicides =
  homicide_data %>% 
  group_by(city_state) %>% 
  summarize(n_obs = n()) %>% 
  arrange(desc(n_obs))
  
knitr::kable(total_homicides, col.names = c('City, State', 'Total Homicides (n)'))
```

The output above reveals a something unexpected in these data: the homicide data collected by _Washington Post_ actually contains data from __51__ distinct cities, as opposed to the 50 originally indicated. Further analysis of the data reveals that this discrepancy likely arose due to the fact that there are two cities named Tulsa in different states: one in Alabama and another in Oklahoma. Although the data from Tulsa, AL seems to suggest that it may be a typographical error (i.e., Tulsa, AL was supposed to be entered as Tulsa, OK) since it reports to have one single homicide over a decade, it is important for data analysts to not make any assumptions about the data. Since the raw data indicates that Tulsa, AL only reported one single homicide, and therefore has minimal impact on our overall sample size, the data from Tulsa, AL will be removed for further analysis. 

```{r}
total_homicides = total_homicides %>% 
  filter(n_obs > 1)
```


#### _Total Unsolved Homicides per City_

To determine the total number of unsolved homicides per city, we can generate a new binary `unsolved_homicide` variable, which will take on a value of 1 if the homicide is unsolved (i.e., disposition = "Closed without arrest" or "Open/No arrest") and 0 if the homicide is solved (i.e., disposition = "Closed by arrest"). We will save the output as a new data frame, to be used in further analysis. 

```{r}
homicide_data = homicide_data %>% 
  mutate(unsolved_homicide = ifelse(disposition == "Closed by arrest", 0, 1))

unsolved_homicides =
  homicide_data %>% 
  group_by(city_state) %>%
  filter(unsolved_homicide == 1) %>% 
  summarize(n_obs = n()) 

knitr::kable(unsolved_homicides, col.names = c('City, State', 'Unsolved Homicides (n)'))
```

## 2.2 City-Level Homicide Porportion Estimates & 95% CIs

#### _Proportion of Unsolved Homicides in Baltimore, MD_

Below we use the `prop.test` function to estimate the proportion of homicides that are unsolved in Baltimore, MD. The arguments for the `prop.test` function, x and n, were determined using the outputs from Problem 2.1. Argument x, a vector of counts of successes, is the number of unsolved homicides in Baltimore, MD, and n, a vector of counts of trials, is the total number of homicides in Baltimore, MD.

The output generated from the `prop.test` function is then saved as an R object. The `broom::tidy()` function was applied to tidy these data, and finally the proportion estimate and the 95% confidence interval values were pulled. 

```{r}
homicide_data %>% 
  filter(city_state == "Baltimore, MD")

prop_test_homicide_baltimore = prop.test(x = 1825, n = 2827, conf.level = 0.95) 

prop_test_homicide_baltimore %>%
  broom::tidy() %>%
  select(estimate, starts_with("conf")) %>% 
  knitr::kable(col.names = c('Proportion Estimate', 'Lower 95% CI Limit', 'Upper 95% CI Limit'))
```

#### _Proportion of Unsolved Homicides in All Cities_

We will now apply the `prop.test` function to all cities in the data set. This process involves first creating a new `homicide_data_to_prop_test` function, which applies the `prop.test` function to any inputs. The `inner_join` function is applied to merge the two data frames containing city-level total and unsolved homicide counts to yield one data frame with both pieces of information. The city-level homicide data will then be nested using the `nest` function to generate list columns for city-level homicide counts, and the `homicide_data_to_prop_test` function will be mapped to each tibble using `purrr::map`. 

```{r}
homicide_data_to_prop_test = function(homicide_data) {
  prop.test(x = homicide_data$n_obs.y, n = homicide_data$n_obs.x, conf.level = 0.95) %>%
    broom::tidy() %>% 
    select(estimate, starts_with("conf"))
}

homicide_data_analysis = inner_join(total_homicides, unsolved_homicides, by = "city_state") %>%
  nest(data = n_obs.x:n_obs.y) %>%
  mutate(data = purrr::map(data, ~ homicide_data_to_prop_test(.x))) %>%
  unnest(cols = data)

knitr::kable(homicide_data_analysis, col.names = c('City, State', 'Proportion Estimate', 'Lower 95% CI Limit', 'Upper 95% CI Limit'))
```

#### _Plotting City-Level Homicides Proportion Estimates & 95% CIs_

using the generated output from the analysis above, we can plot the city-level proportion estimates of unsolved homicides using `ggplot`. In addition to plotting the proportion estimates using `geom_point`, the plot below also depicts the 95% confidence intervals associated with each estimate, which were applied using `geom_errorbar`. Cities are ordered in increasing proportion estimates of unsolved homicides. 

```{r}
homicide_data_analysis %>% 
  ggplot(aes(x = reorder(city_state, estimate), y = estimate, color = city_state)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "City-Level Proportion Estimates for Unsolved Homicides", 
    x = "City, State", 
    y = "Proportion Estimate") + 
  theme(
    axis.text.x = element_text(angle = 70, hjust = 1), 
    legend.position = "none")
```

# Problem 3

## 3.1 One-Sample T-Test Simulation Setup

#### _Setting Model Design Elements_

To conduct a simulation exploring power in a one-sample t-test, we will first create a new `ttest_sim` function and set the following model design elements as prescribed: sample size (n) = 30; and standard deviation = 5.

```{r}
ttest_sim = function(n = 30, mu, sigma = 5) {
  tibble(x = rnorm(n = n, mean = mu, sd = sigma)) %>%
    t.test() %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
}
```

#### _Generating Data Sets for Mu's_

Below we apply the `ttest_sim` function for mu = {0, 1, 2, 3, 4, 5, 6}. To do so, we first create a data frame with the range of values for mu, then rerun the `ttest_sim` function on each value of mu 5000 times to generate 5000 data sets, which are stores as list columns for each value of mu in the data frame. We then `unnest` the initial output to generate a list of tibbles for each value of mu. Finally, we `unnest` the output again to see the estimates and p.values corresponding to each value of mu. 

```{r}
ttest_sim_output =
  data.frame(mu = 0:6, output = 0:6) %>%
    mutate(output = purrr::map(.x = output, ~ rerun(5000, ttest_sim(mu = .x)))) %>%
    unnest(cols = output) %>%
    unnest(cols = output)
```

## 3.2 Plotting Simulation Analysis Results

#### _Plotting Power Against True Value of Mu_

With the data sets generated from Problem 3.1 above, we can now plot results. In this first plot, we use `ggplot` to show the proportion of times the null was rejected (i.e. power) on the y-axis and the true value of mu on the x axis. Power was determined by creating a binary variable `null_is_rejected` which equal 1 when the p-value is less than alpha = 0.05. By the definition of power,`power` is equal to the mean of `null_is_rejected`. 

```{r}
ttest_sim_output %>%
  mutate(null_is_rejected = ifelse(p.value < 0.05, 1, 0)) %>%
  group_by(mu) %>%
  summarize(power = mean(null_is_rejected)) %>%
  ggplot(aes(x = mu, y = power)) +
  geom_smooth(se = FALSE) +
  geom_point() + 
  labs(
    title = "Plot of Power Against True Value of Mu", 
    x = "True Value of Mu", 
    y = "Power") +
  scale_x_continuous(n.breaks = 7)
```

Based on the output above, we can see that as the effect size (mu) increases, study power also increases. The increase in power is much more prominent as we go from an effect size of 0 to an effect size of 3, and plateaus when we increase the effect size from 3 to 4, 5, and 6. 

#### _Plotting Average Value of Mu Against True Value of Mu_

In this second plot, we continue to use `ggplot` to show the two following overlaid plots: 

1. Average estimate of mu on the y axis and the true value of mu on the x axis
1. Average estimate of mu _only in samples for which the null was rejected)_ on the y axis and the true value of mu on the x axis

Using a similar approach as above, we can determine the average values of mu by applying the `mean` function to the generated estimates, and adding an additional condition of p-value less than 0.05 when determining the average value of mu for samples where the null was rejected. 

```{r}
ttest_sim_output %>%
  group_by(mu) %>%
  summarize(mean_mu_hat = mean(estimate), mean_rejected_null_mu_hat = mean(estimate[p.value < 0.05])) %>%
  ggplot() +
  geom_point(aes(x = mu, y = mean_mu_hat, colour = 'red')) +
  geom_point(aes(x = mu, y = mean_rejected_null_mu_hat, color = 'blue')) + 
  labs(
    title = "Plot of Average Mu Against True Mu", 
    x = "True Value of Mu", 
    y = "Average Value of Mu") +
  scale_x_continuous(n.breaks = 7) + 
  scale_color_discrete(name = "Sample", labels = c("Null Rejected Samples", "All Samples"))
```

Based on the plot above, we can see that the sample average of mu across tests for which the null is rejected is approximately equal to the true value of mu for mu = {0, 4, 5}. For mu = {1, 2, 3}, there is some deviation from the sample average of mu across tests for which the null is rejected and the true value of mu.
